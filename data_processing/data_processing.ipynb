{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "- data/参考数据集.json中包含有17本中外小说的中文版\n",
    "- 针对每本小说进行每块不超过800字的切分，将切分的块作为输出\n",
    "- 使用deepspeek API对每块文字进行总结，将总结作为数据的输入\n",
    "\n",
    "数据格式如下：\n",
    "```\n",
    "{\n",
    "        \"instruction\": \"你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。\",\n",
    "        \"input\": \"班纳特太太得知尼日斐花园被一个有钱的单身汉彬格莱租下，希望他能成为她女儿们的理想丈夫，并计划让班纳特先生去拜访他。\",\n",
    "        \"output\": \"凡是有钱的单身汉，总想娶位太太，这已经成了一条举世公认的真理。这样的单身汉，每逢新搬到一个地方，四邻八舍虽然完全不了解他的性情如何，见解如何，可是，既然这样的一条真理早已在人们心目中根深蒂固，因此人们总是把他看作自己某一个女儿理所应得的一笔财产。\\n有一天班纳特太太对她的丈夫说：“我的好老爷，尼日斐花园终于租出去了，你听说过没有？”班纳特先生回答道，他没有听说过。\\n“的确租出去了，”她说，“朗格太太刚刚上这儿来过，她把这件事的底细，一五一十地告诉了我。”班纳特先生没有理睬她。\\n“你难道不想知道是谁租去的吗？”太太不耐烦地嚷起来了。\\n“既是你要说给我听，我听听也无妨。”这句话足够鼓励她讲下去了。\\n“哦！亲爱的，你得知道，郎格太太说，租尼日斐花园的是个阔少爷，他是英格兰北部的人；听说他星期一那天，乘着一辆驷马大轿车来看房子，看得非常中意，当场就和莫理斯先生谈妥了；他要在‘米迦勒节’以前搬进来，打算下个周未先叫几个佣人来住。”“这个人叫什么名字？”“彬格莱。”“有太太的呢，还是单身汉？”“噢！是个单身汉，亲爱的，确确实实是个单身汉！一个有钱的单身汉；每年有四五千磅的收入。真是女儿们的福气！”“这怎么说？关女儿女儿们什么事？”“我的好老爷，”太太回答道，“你怎么这样叫人讨厌！告诉你吧，我正在盘算，他要是挑中我们一个女儿做老婆，可多好！”“他住到这儿来，就是为了这个打算吗？”“打算！胡扯，这是哪儿的话！不过，他倒作兴看中我们的某一个女儿呢。他一搬来，你就得去拜访拜访他。”“我不用去。你带着女儿们去就得啦，要不你干脆打发她们自己去，那或许倒更好些，因为你跟女儿们比起来，她们哪一个都不能胜过你的美貌，你去了，彬格莱先生倒可能挑中你呢？”“我的好老爷，你太捧我啦。从前也的确有人赞赏过我的美貌，现在我可有敢说有什么出众的地方了。一个女人家有了五个成年的女儿，就不该对自己的美貌再转什么念头。\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json\n",
    "from functools import reduce\n",
    "import jieba\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理\n",
    "\n",
    "数据预处理步骤：\n",
    "1. 读取数据\n",
    "2. 数据分块\n",
    "3. 使用deepspeek API对每块文字进行总结并保存数据\n",
    "4. 数据合并\n",
    "\n",
    "### 2.1 读取数据\n",
    "data/参考数据集.json中数据格式如下\n",
    "```\n",
    "{\n",
    "    \"name\": \"三国演义\",\n",
    "    \"len\": 593514,\n",
    "    \"dir\": \"./douban_yamaxun//D-三国演义-10.json\",\n",
    "    \"text\": ...\n",
    "}\n",
    "{\n",
    "    \"name\": \"水浒传\",\n",
    "    \"len\": 852570,\n",
    "    \"dir\": \"./douban_yamaxun//D-水浒传-10.json\",\n",
    "    \"text\": ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 读取参考数据集\n",
    "novel_data = []  # 将读取到的小说数据存储在这个列表中\n",
    "with open('/Users/huangxinzhe/LLM/ali_llm_fine_tuning_competition/data/参考数据集.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        novel_data.append(json.loads(line))  # json文件中每行是一本小说\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《三国演义》的字数为： 593514 字\n",
      "《水浒传》的字数为： 838774 字\n",
      "《儒林外史》的字数为： 327064 字\n",
      "《呼啸山庄》的字数为： 215931 字\n",
      "《百年孤独》的字数为： 246912 字\n",
      "《西游记》的字数为： 714847 字\n",
      "《红与黑》的字数为： 361019 字\n",
      "《战争与和平》的字数为： 1056718 字\n",
      "《聊斋志异》的字数为： 384606 字\n",
      "《醒世恒言》的字数为： 328581 字\n",
      "《傲慢与偏见》的字数为： 218614 字\n",
      "《红楼梦》的字数为： 789372 字\n",
      "《隋唐演义》的字数为： 636063 字\n",
      "《封神演义》的字数为： 588226 字\n",
      "《拍案惊奇》的字数为： 238248 字\n",
      "《尤利西斯》的字数为： 363155 字\n",
      "《福尔摩斯探案集》的字数为： 363914 字\n"
     ]
    }
   ],
   "source": [
    "# 查看数据集中所有小说的名字和字数\n",
    "for i in novel_data:\n",
    "    print(f\"《{i['name']}》的字数为： {len(i['text'])} 字\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "本次比赛中小说创作任务如下\n",
    "任务中没有要求生成文言文小说，后续训练模型时可以进行对比实验，考虑数据中是否要加入文言文的小说\n",
    "\n",
    "\n",
    "1.1现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程\n",
    "1.2一个现代女性穿越到古代某朝代后发生的传奇故事\n",
    "1.3现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事\n",
    "1.4古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事\n",
    "1.5现代玄幻背景，在一所驯服神兽的魔法学校中，围绕着三个学生小伙伴发生的奇幻冒险故事\n",
    "1.6古代侦探系列，一位才华横溢的年轻学士，在解决一连串神秘案件中揭露皇室阴谋的故事\n",
    "1.7二十一世纪初，一个小镇上发生的一系列神秘事件，让一群青少年开始探索超自然现象，并发现了小镇隐藏的古老秘密的故事\n",
    "1.8现代都市背景，一个名不见经传的漫画家，通过与自己创作的虚拟角色“交流”，解决一系列诡秘案件的故事\n",
    "1.9古代异界背景，一位天赋异禀的少年，在师傅的指导下学习古老的灵术，最终踏上寻找失落的神器，拯救家园的冒险旅程的故事\n",
    "1.10繁华都市背景，一个单亲妈妈如何在抚养孩子和维持生计之间找到平衡，同时保持对自己梦想的追求的故事\n",
    "1.11现代悬疑系列，一位心理学家利用自己的专业知识，帮助警方侦破一系列复杂的心理游戏案件\n",
    "1.12现代心理惊悚背景，一名精神科医生被卷入一连串的脑控实验阴谋，如何在精神与现实的边缘徘徊求生的故事\n",
    "1.13虚构古代背景，一位年轻的书生因缘巧合获得一本神秘典籍，开启了他成为一代宗师的修道之旅\n",
    "1.14古代神话背景，一位勇者如何经过重重试炼，最终获取神器，拯救世界于水深火热之中的传奇故事\n",
    "1.15虚拟现实背景，一群玩家在一款极度真实的VR游戏中探索未知世界并揭露游戏背后隐藏的秘密的故事\n",
    "1.16穿越时空背景，一群来自不同时代的人意外聚集在一个神秘的地方，他们如何互相协作，解开时空之谜的故事\n",
    "1.17科幻背景，一个机器人意识觉醒后，它如何在追求自我身份的同时，挑战人类社会关于存在和自由的根本问题\n",
    "1.1820世纪60年代的欧洲，一个侦探在解决一起跨国艺术品盗窃案中，逐渐揭露出一个关于失落宝藏的大阴谋\n",
    "1.19现代都市背景，一位因交通事故失去双腿的舞者，通过先进的义肢技术重新站起来，重新找回舞台与自我的故事\n",
    "1.20古代背景，一个普通医女奋斗成为朝廷高官，最终影响整个王朝政治格局变化的故事\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集中第 4 本小说《呼啸山庄》的文本作为训练集数据来源\n",
    "data = novel_data[3][\"text\"]\n",
    "story_name = novel_data[3][\"name\"]\n",
    "# 查看《呼啸山庄》全文\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数据分块\n",
    "- 将每本小说分块，每块不超过800字\n",
    "- 想要保证语义完整性，可以使用滑动窗口的方式进行分块（滑动窗口即：每段文本之间有重叠的部分）\n",
    "\n",
    "上述处理数据的方式微调优缺点如下：\n",
    "- 因为对原始文本进行800切分，因此在生成文本时对文本字数有天然的限制\n",
    "- 因为是截取的文本，因此文本的开头和结尾可能不完整\n",
    "\n",
    "在条件允许的情况下，推荐使用不超过800字的短篇小说制作数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用jieba进行句子切分\n",
    "sentences = []\n",
    "\n",
    "for sentence in data.split('。'):  # 使用句号作为切分符\n",
    "    sentences.append(sentence)  # 存储在列表中的语句都是没有句号的\n",
    "\n",
    "# 将句子合并成800字一段的段落\n",
    "paragraphs = []\n",
    "current_paragraph = ''\n",
    "for sentence in sentences:\n",
    "    if len(current_paragraph) + len(sentence) <= 800:  # 如果当前段落加上当前句子长度不超过800字\n",
    "        current_paragraph += sentence+'。'  # 将当前句子加入到当前段落最后\n",
    "    else:\n",
    "        paragraphs.append(current_paragraph.strip())  # 当前段落长度超过800字，将当前段落加入到段落列表中\n",
    "        current_paragraph = sentence  # 将当前句子作为新的段落\n",
    "\n",
    "# 将最后一段加入到段落列表中\n",
    "if current_paragraph:\n",
    "    paragraphs.append(current_paragraph.strip())\n",
    "\n",
    "# 打印切分后的段落\n",
    "for idx, paragraph in enumerate(paragraphs):\n",
    "    print(f'段落 {idx + 1}: {paragraph}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 使用deepspeek API对每块文字进行总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置loguru输出到文件\n",
    "logger.remove()  # 移除默认的控制台输出\n",
    "logger.add(\"logs/app_{time:YYYY-MM-DD}.log\", level=\"INFO\", rotation=\"00:00\", retention=\"10 days\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepspeek-chat api接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用deepseek-chat api给段落打标签的接口\n",
    "def get_response(text):\n",
    "    client = OpenAI(\n",
    "        api_key=DEEPSEEK_API_KEY,  # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n",
    "        base_url=\"https://api.deepseek.com\",  # 填写DashScope SDK的base_url\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'system', \n",
    "                'content': '总结user提交的内容。用一句不超过50字的话总结这段小说的情节。仅回答总结，不需要添加其他内容。'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user', \n",
    "                'content': text\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用日志打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置容错机制，可最多重试 5 次，如果失败记录错误日志\n",
    "def get_summary_with_retry(text):\n",
    "    max_retries = 5\n",
    "    retry_delay = 15  # in seconds\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            return get_response(text)\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_retries:\n",
    "                logger.warning(f\"Attempt {attempts} failed for text: {text}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                logger.error(f\"All {max_retries} attempts failed for text: {text}. Error: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建文件夹\n",
    "os.makedirs('data', exist_ok=True)  # 存放每部小说切分后的数据josn文件\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('dataset', exist_ok=True)  # 存放所有小说json文件的合并文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多线程加速处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用线程池进行多线程访问，并控制提交任务的速度\n",
    "def process_texts(texts):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:  # 设置最大线程数为16\n",
    "        future_to_text = {}\n",
    "        for text in tqdm(texts, desc=\"Submitting tasks\", total=len(texts)):  # 使用tqdm显示进度条，total参数设置总任务数，desc参数设置进度条前缀\n",
    "            future = executor.submit(get_summary_with_retry, text)  # 提交任务到线程池\n",
    "            future_to_text[future] = text  # 将future和text的映射存储在字典中\n",
    "            time.sleep(0.2)  # 控制每0.2秒提交一个任务\n",
    "        for future in tqdm(as_completed(future_to_text), total=len(texts), desc=\"Processing tasks\"):\n",
    "            text = future_to_text[future]\n",
    "            try:\n",
    "                summary = future.result()  # 获取任务的结果\n",
    "                results.append((text, summary))  # 将任务的输入和输出存储在results列表中\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process text: {text}. Error: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量给指定的小说打标签的接口函数\n",
    "def build_dataset(novel,texts):\n",
    "    instruction_prompt = \"你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。\"\n",
    "    dataset = []\n",
    "    dataset_error = []\n",
    "\n",
    "    # 使用多线程处理文本\n",
    "    processed_texts = process_texts(texts)\n",
    "\n",
    "    for text, summary in processed_texts:\n",
    "        if summary:\n",
    "            dataset.append({\n",
    "                \"instruction\": instruction_prompt,\n",
    "                \"input\": summary,\n",
    "                \"output\": text\n",
    "            })\n",
    "        else:\n",
    "            dataset_error.append(text)\n",
    "    \n",
    "    with open(f\"./data/{novel}.json\", \"w\") as f:\n",
    "        f.write(json.dumps(dataset, ensure_ascii=False, indent=4))\n",
    "\n",
    "    with open(f\"./data/{novel}_error.txt\", \"w\") as f:\n",
    "        f.write(json.dumps(dataset_error, ensure_ascii=False, indent=4))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给指定的文本打标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = novel_data[3][\"text\"]\n",
    "story_name = novel_data[3][\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始给段落打标签\n",
    "dataset = build_dataset(story_name,paragraphs[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给数据集中所有的文本打标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in novel_data:\n",
    "    story_name = data[\"name\"]\n",
    "    data = data[\"text\"]\n",
    "\n",
    "    sentences = []\n",
    "    for sentence in data.split('。'):\n",
    "        sentences.append(sentence)\n",
    "    paragraphs = []\n",
    "    current_paragraph = ''\n",
    "    for sentence in sentences:\n",
    "        if len(current_paragraph) + len(sentence) <= 800:\n",
    "            current_paragraph += sentence+'。'\n",
    "        else:\n",
    "            paragraphs.append(current_paragraph.strip())\n",
    "            current_paragraph = sentence\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(current_paragraph.strip())\n",
    "        \n",
    "    dataset = build_dataset(story_name,paragraphs[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 数据合并\n",
    "如果有多个小数数据请处理完毕后执行此步骤，将data下的所以json数据文件整理到dataset的merged_story.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 设置文件夹路径\n",
    "directory_path = '/Users/huangxinzhe/LLM/ali_llm_fine_tuning_competition/data_processing/data'\n",
    "\n",
    "# 初始化一个空列表，用于存储合并后的数据\n",
    "merged_data = []\n",
    "\n",
    "# 遍历文件夹下的所有文件\n",
    "for filename in os.listdir(directory_path):\n",
    "    # 检查文件扩展名是否为.json\n",
    "    if filename.endswith('.json'):\n",
    "        # 构建文件的完整路径\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        # 打开并读取JSON文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # 加载JSON内容到变量\n",
    "            data = json.load(file)\n",
    "            # 将当前文件的数据添加到合并列表中\n",
    "            merged_data.extend(data)\n",
    "\n",
    "# 将合并后的数据转换为JSON格式\n",
    "merged_json = json.dumps(merged_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 可以选择将合并后的数据写入到一个新的JSON文件中\n",
    "output_file_path = '/Users/huangxinzhe/LLM/ali_llm_fine_tuning_competition/data_processing/dataset/merged_story.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(merged_json)\n",
    "\n",
    "# 或者直接输出到控制台\n",
    "print(merged_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
